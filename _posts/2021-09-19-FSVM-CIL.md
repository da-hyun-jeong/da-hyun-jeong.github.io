---
title: "FSVM-CIL : Fuzzy Support Vector Machines for Class Imbalance Learning"
excerpt: fuzzy SVM for CIL
toc: true
toc_sticky: true

author_profile: false

date: 2021-09-09 15:10:00 -0000
categories: 
  - paper review
tags:
  - Machine Learning
---

<br>

2021.09.09 세미나 발표 논문 정리. <br>
이상치가 존재하고, 데이터 클래스가 불균형할 때 사용할 수 있는 FSVM-CIL 방법론에 대한 논문입니다. <br>
이전 review paper를 잘 이해한다면 아주 쉽게 따라갈 수 있습니다.
<https://ieeexplore.ieee.org/abstract/document/5409611>

<br>

# Abstract
SVN은 balanced data(2-class 문제에서 충분한 데이터가 존재)에서 잘 작동하지만, imbalanced data에서는 최적의 분류 모델을 생성하지 못한다. <br>
기존의 class imbalanced learning(CIL) 방법은 class 불균형을 덜 민감하게 만들 수 있지만 outlier, noise에는 여전히 민감하다. <br>
fuzzy SVM은 outlier, noise 문제를 해결하기 위해 제안된 방법론이지만 standard SVM과 마찬가지로 class 불균형 문제가 존재한다. <br>
따라서 저자는 outlier가 존재할 때 class 불균형 문제를 처리할 수 있는 방법론인 **FSVMs for CIL(called FSVM-CIL)**을 제안한다. <br>
여러 dataset에서 FSVM-CIL과 기존의 CIL 방법론들을 비교함으로써 outlier, noise가 존재할 때 제안한 방법론이 매우 효과적인 방법임을 보인다.


<br>
<br>

# Introduction


SVM은 탄탄한 수학적 이론을 바탕으로 global classification solution을 찾을 수 있어 높은 generalization ability를 가진다. <br>
그러나 불균형 데이터셋에서는 개체가 많은 클래스에 치우치게(biased)되므로 개체가 적은 클래스에서는 성능이 떨어진다. <br>

이 논문에서는 + class는 소수, - class는 다수로 취급한다. <br>

CIL method는 크게 externel methods, internal methods가 있다.
- externel : 데이터셋 전처리
- internel : 학습 알고리즘 수정(보통 목적함수 수정)

<br>
F-SVM은 각 데이터의 중요도를 반영하기 위해 'fuzzy membership'을 할당하는 방식으로 최적의 분리 초평면을 찾을 때 outlier/noise의 영향을 줄인다.

<br>
<br>

# standard SVM (skip)

이전 포스팅을 참고해주세요 !

<br>
<br>

# SVM에 적용할 수 있는 CIL 방법

> externel methods

1. resampling method
- undersampling : 클래스 비율이 특정한 값이 될 때까지 다수 클래스의 example 제거
- oversampling : 클래스 비율이 특정한 값이 될 때까지 소수 클래스의 example 중복

2. 클러스터 기반 sampling
3. genetic algorithm
4. boosting
   
<br>

> internel methods

- Differnet Error Costs(DEC)
  
SVM의 목적함수가 두 개의 오분류 비용 값 C+와 C-를 할당하도록 수정된 **서로 다른 오류 비용(DEC)**이라는 방법이 있다.

$$
\begin{aligned}
\textrm{minimize}_{w,b} \quad & \frac{1}{2}w^{T}w+C^+\sum_{[i|y_i=+1]}^{n}{\epsilon_i}+C^-\sum_{[i|y_i=-1]}^{n}{\epsilon_i}\\
\textrm{subject to} \quad & y_i(w^T\phi(x_i)+b) \geq1-\epsilon_i ,  \quad \textrm{for} \; i=1,...,n\\
  &\epsilon_i\geq0  ,  \quad \textrm{for} \; i=1,...,n  \\
\end{aligned}
$$

오분류에 대한 temr을 하나로 보는 것이 아니라 + class와 - class로 나누어 cost value를 계산한다. <br>
소수 class에 대해 더 높은 cost value를 할당해서 불균등 영향을 줄인다. ( $C^+ > C^-$ )

<br>

- zSVM

이 방법에서는 먼저 주어진 불균형 데이터셋으로 standard SVM을 적합시킨다. <br>
그런 다음, 얻어진 결정 경계를 수정하여 다수 class에 대한 분류기의 bias을 제거한다. <br>
모든 + suppor vector에 특정한 작은 양수 값 $z$를 곱하여 영향력(계수 크기)를 증가시킨다. <br>
수정된 decision function은 다음과 같다.

$$
\begin{aligned}
  y(x) &= sign(w^T\phi(x) + b) \quad (x : new) \\
  &= sign(\sum_{i=1}^{n}{\alpha_i y_i  K(x, x_i) + b}) \\
  &= sign(\sum_{i=1}^{n_1}{\alpha_i^+ y_i  K(x, x_i)} + \sum_{i=1}^{n_2}{\alpha_i^- y_i  K(x, x_i)} + b )  \\
  & \Rightarrow sign(z\sum_{i=1}^{n_1}{\alpha_i^+ y_i  K(x, x_i)} + \sum_{i=1}^{n_2}{\alpha_i^- y_i  K(x, x_i)} + b )
\end{aligned}
$$

여기서 $\alpha$는 각 클래스의 support vector의 계수이고, $n_1$과 $n_2$는 각 클래스의 훈련 관측치 갯수이다. <br>
$\alpha^+$에 대해 이러한 수정을 해주면 의사결정 함수에서 소수 클래스의 support vector 가중치를 증가시킬 것이고, 따라서 다수 클래스에 대한 편향을 감소시킬 것이다.

<br>

- 이외 kernel SVM의 수정 버전 존재

<br>
<br>

# fuzzy SVM

> fuzzy theory

fuzzy set은 기본적으로 확률과 다르다. <br>
개체가 각 클래스에 속한다, 속하지 않는다 하는 이진법 논리에서 벗어나서 클래스 속하는 정도를 **'membership function'**으로 나타낸다. <br>
말 그대로 이것도 될 수 있고, 저것도 될 수 있다는 뜻으로 흑백의 스펙트럼이 넓어 '회색이론' 이라고 부르기도 한다. <br>
주로 공학분야에서 가전제품, 제어시스템 쪽에서 많이 사용되는 이론이다.

<br>

> FSVM-soft margin 최적화

$$
\begin{aligned}
\textrm{minimize}_{w,b} \quad & \frac{1}{2}w^{T}w+C\sum_{i=1}^{n}{m_i\epsilon_i}\\
\textrm{subject to} \quad & y_i(w^T\phi(x_i)+b) \geq1-\epsilon_i ,  \quad \textrm{for} \; i=1,...,n\\
  &\epsilon_i\geq0  ,  \quad \textrm{for} \; i=1,...,n  \\
\end{aligned}
$$

멤버십 $m_i$가 작을수록 오분류 기여도를 나타내는 $\epsilon_i$의 영향이 작아지고, 대응되는 $x_i$의 중요도 역시 작아진다. <br>
오분류에 대한 cost $C$의 관점으로 보면 중요한 개체일수록 cost가 높아지게 된다. <br>
lagrangian dual form에서 기존 SVM과의 차이는 $\alpha_i$의 upper bound만 다르다. <br>
이전의 연구들에서 FSVM 방법론이 outlier와 noise의 영향을 줄이는 데 잘 적용되었다고 나와있다.

<br>
<br>

# FSVM-CIL

> fuzzy membership idea

멤버십을 할당하는 것은 training data의 오분류 cost로 $Cm_i$를 할당하는 것과 같다. <br>
class 불균형을 줄이려면 소수 클래스에 더 높은 멤버십/cost를 할당해야 한다. ( $m^+>m^-$ ) <br>
이것은 앞에서 CIL 방법 중 'Different Error Costs'를 적용한 것이다. 

<br>

우선 각 class에서 멤버십은 다음과 같이 정의된다.

$$
m_i^+ = f(x_i^+)r^+ \\
m_i^- = f(x_i^-)r^-
$$

여기서 $f(x_i)$는 구간 (0,1) 사이 값을 생성하는 함수이고, $r^+>r^-$은 class imbalance value이다. <br>
이렇게 정의해주면 $m^+$는 [0 , $r^+$] 사이값이고, $m^-$는 [0 , $r^+$] 사이값이 생성된다. 

<br>

class imbalance를 반영하기 위해서 $r^+$=1, $r^-$=$r$로 정의한다. <br>
여기서 $r$은 $\frac{+ \; class}{- \; class}$ 비율이다. <br>
이 정의는 DEC 방법에서 $\frac{C^+}{C^-}$ 값이 $\frac{+ \; class}{- \; class}$ 비율과 동일한 맥락이라는 사전 연구 결과에 따른 것이다. <br>
이렇게 정의하면 $m^+$는 [0, 1] 사이값,  $m^-$는 [0, $r$] 사이값이 된다.

<br>

> membership function $f(x_i)$ 

training sample의 관측치들이 class 내에서 중요도를 결정하는 함수를 정의하는 방법으로 다음의 3가지를 제시하고 있다.

- 개체와 속한 class 중심과의 거리

$f(x_i)=d_i^{cen}$이라고 정의하자. 

$$
d_i^{cen} = ||x_i -\bar{x} ||^{1/2}  \quad (l_2 \; norm)
$$

$d_i^{cen}$은 중심에 가까울수록 더 많은 정보를 갖는다고 볼 수 있으므로 더 높은 값이 할당되고, 중심과 멀수록 이상치로 간주할 수 있으므로 더 낮은 값이 할당된다. <br>

이 함수로 2개의 감소함수를 정의한다.
$$
f_{lin}^{cen}(x_i)=1-\frac{d_i^{cen}}{max(d_i^{cen})+\Delta} : linearly \; decaying \; function \;  \\
f_{exp}^{cen}(x_i) = \frac{2}{1+exp(\beta d_i^{cen})}: exponetially \; decaying \; function \; 
$$

여기서 $\Delta$는 작은 양수값이고, $\beta$는 지수적으로 감소하는 속도를 조절하는 파라미터이다.

<br>

- 개체와 추정된 초평면 간의 거리

여기서 자세하게 언급은 안하지만 noise distribution을 정의해서 초기 SVM 모델을 추정한다.

$f(x_i)=d_i^{sph}$이라고 정의하자. 

$$
d_i^{sph} = ||x_i -\bar{x} ||^{1/2}  \quad (l_2 \; norm)
$$

spherical region은 두 클래스 간의 교집합 영역을 포괄하는 hypersphere를 말한다.<br>
이 영역에서 초평면이 통과할 가능성이 높다. <br>
여기서 $\bar{x}$는 구영역의 중심이다. <br>

이 함수로 2개의 감소함수를 정의한다.
$$
f_{lin}^{sph}(x_i)=1-\frac{d_i^{cen}}{max(d_i^{sph})+\Delta} \\
f_{exp}^{sph}(x_i) = \frac{2}{1+exp(\beta d_i^{sph})} 
$$

<br>

- 개체와 실제 초평면과의 거리

불균형 training data에 standard SVM을 적용해서 얻은 초평면과의 거리이다. <br>
분리 초평면과 가까우면 많은 정보를 가지므로 높은 멤버십 값을 준다. <br>
구한 초평면에 대해 각 개체 $x_i$의 functional margin을 구하는데 이는 decision function value의 절댓값이다. 

<br>

$f(x_i)=d_i^{hyp}$이라고 정의하자. <br>

$$
d_i^{sph} = w^T\phi(x_i)+b
$$

이 함수로 2개의 감소함수를 정의한다.
$$
f_{lin}^{hyp}(x_i)=1-\frac{d_i^{cen}}{max(d_i^{hyp})+\Delta} \\
f_{exp}^{hyp}(x_i) = \frac{2}{1+exp(\beta d_i^{hyp})} 
$$

<br>
<br>

# 실험 결과

- 위에서 제시된 6개의 함수를 이용
- multiclass를 binary로 축소 **(실험의 한계점)**
- 모델 평가 방법 : 불균형 데이터이므로 민감도, 특이도의 기하평균을 척도로 사용
- 5 fold CV, 불균등 비율이 일정하도록 층화추출
  
- 실험결과 : 기존의 CIL 방법만 사용했을 때, fuzzy SVM을 사용했을 때보다 성능이 좋다.

<br>
<br>

# Comment


class imbalance 문제는 기존의 로지스틱 모델에서도 화두였다고 하는데 난 이 논문을 통해 처음 이 키워드를 접하게 되었다. <br>
이상치가 존재할 때도 imbalance 문제를 해결할 수 있다는 것이 정말 좋은 아이디어같다. <br>
다른 머신러닝 방법론 중에 boosting 쪽에는 어떻게 활용되는지 더 찾아봐야겠다. <br>
조금 아쉬운 점은 CIL 방법에 대해 소개만 해주고 있어서 자세하게 알 수는 없었다..! 