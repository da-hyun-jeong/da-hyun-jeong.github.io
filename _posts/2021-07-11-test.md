---
title: "A New Fuzzy Support Vector Machine to Evaluate Credit Risk"
excerpt: test subtitle
toc: true
toc_sticky: true

author_profile: false

date: 2021-08-20 19:40:00 -0000
categories: 
  - Credit Scoring
tags:
  - Credit Risk Evaluation
  - Machine Learning
---

2021.08.20 세미나 발표 논문 정리. <br>
standard SVM 보다 fuzzy SVM에 대한 직접적인 내용을 다룹니다. <br>
<https://ieeexplore.ieee.org/abstract/document/1556587>

<br>
<br>

# Introduction
은행, 신용보조기관은 신용 리스크를 관리하는 것이 매우 중요하다.<br>
돈을 빌려주고 받는 이자로 수익을 내는 것도 중요하지만, 고객의 파산, 채무불이행 리스크도 고려해야 한다.<br>
무엇보다 세심하게 평가되지 않은 신용등급, 신용점수로 인해 고객의 대출승인심사가 거절되는 등의 피해는 없어야 한다.


신용평가 영역에서는 'good creditor', 'bad creditor'을 절대적으로 나눌 수 없다. <br>
이러한 직관을 반영하여 단순히 class를 나누는 것이 아니라 membership(class에 속할 가능성, 점수와 같은 것)을 부여해서 'good'과 'bad' 일 가능성을 열어두면 좀 더 실용적인 모델이 될 것이라 기대할 수 있다.

<br>

# SVM
SVM은 기본적으로 class를 분류하는 문제이다. <br>
우리가 접하는 데이터는 대게 깔끔하게 class를 분리할 수 없다. <br>
original space에서는 분리할 수 없을 때, 고차원으로 mapping 해주고 직선이 아닌 평면으로 분리해주자는 아이디어를 생각하자. <br>
이 때 선형으로 분리가능한 hyperplane을 찾는 것이 목적이다. <br>
데이터와 hyperplane의 거리를 최대화하면 최적의 hyperplane을 구할 수 있다. <br>
각 class에서 가장 가까운 관측치 들 간의 거리를 margin이라 하고, 초평면 & 반공간에 대한 분류식에 의해 SVM은 margin을 최대화하는 convex 문제가 된다. <br>

<br>

> Hard Margin

다음과 같은 training sample을 고려해보자. <br>

$$ (x_k, y_k) , \quad  k=1,..,n \\
where \quad x_k \in R^d, \quad  y_k \in (1,-1) $$

$x_k$ 는 고객의 특성이라고 볼 수 있고, $\,y_k$는 대출승인 여부, 채무불이행 여부 등이 될 수 있다. <br>
training sample은 비선형함수 $\phi(\bullet)$ 을 이용해 고차원으로 mapping 했을 때 선형으로 분리가능하다고 가정한다. <br>
분류기는 다음과 같이 표현된다.

$$
\begin{cases}
    w^T\phi(x) + b \geq 1  & \text{if }  y_k=1 \\
    w^T\phi(x) + b \leq -1, & \text{if }  y_k=-1 \\
\end{cases}
$$

$x^+$을 + plane 위의 점, $x^-$ 을 - plane 위의 점이라고 하자. <br>
두 점은 $x^+=x^- + \lambda w$의 평행이동 관계가 있으므로 plane 간의 거리는 $2/w^Tw$가 된다.

$$
w^Tx^+ b = 1 \\
\begin{aligned}
  \Rightarrow w^T(x^- + \lambda w) + b = 1 \\
  \Rightarrow w^Tx^- + b + \lambda w^Tw = 1 \\
  \Rightarrow -1 + \lambda w^Tw = 1 \\
  \Rightarrow \lambda = \frac{2}{w^Tw} 
\end{aligned}
$$


이 식을 이용해서 Margin을 구하면 $2/||w||_2$가 된다.


$$
\begin{aligned}
margin &= distance(x^+, x^-) \\
&= ||x^+- x^-||_2 \\
&= || (x^- + \lambda w) - x^-||_2 \\
&= || \lambda w||_2 \\
&= \lambda \sqrt{w^Tw} \\
&= \frac{2}{w^Tw} \sqrt{w^Tw} \\
&= \frac{2}{\sqrt{w^Tw}} = \frac{2}{||w||_2}

\end{aligned}


$$

목적함수는 이 margin을 최대화하는 것이고 1/margin을 최소화하는 문제와 같아진다. <br>
계산의 편의를 위해 일대일함수인 제곱형태로 수정해준 목적함수를 사용한다.

$$
min \, J(w,b) = min \, \frac{1}{2} ||w||_2^2
$$

<br>

> soft margin

살제 데이터는 고차원으로 mapping 해도 선형분리가 되지 않는 경우가 많다. <br>
즉, 데이터를 완전히 분리하는  hyperplane을 찾을 수 없다. <br>
약간의 오류를 허용하기 위해 error term $\xi_k$ 를 도입하면 다음과 같은 최적화 문제가 된다. <br>

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{\xi_k}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

<br>

정규화 모수 C는 2가지 목표를 trade-off 하는 상수이다. <br>
이상치 존재 가능성을 낮게 봐서 error term을 최소화하는데 집중하는 경우 large C를 사용하고, <br>
이상치 존재 가능성을 크게 봐서 margin을 최대화하고 싶은 경우 small C를 사용한다.

lagrange multiplie를 이용하여 


> kernel SVM

$\phi(x)$는 input vector를 고차원으로 mapping 해주는 함수이다. <br>
그 차원에서 두 개의 class가 선형분리된다. <br>
QP solution을 구하는 데 필요한 $\phi(x)$의 정확한 형태를 알기 어렵다.
kernel SVM은 이를 해결하기 위해  facture space에서 내적 형태인 kernel function을 정의하고, 이 함수로 feature space에서 데이터를 선형으로 분리한다.
따라서 $\phi(x)$를 몰라도 최적의 mapping 함수를 찾을 수 있다.

$$
K(x_i , x_j) = \phi(x_i)^T\phi(x_j)
$$

커널이 Mercer의 조건을 만족하면 위와 같은 형태의 $\phi(x)$가 존재한다.
$\phi(x_i)^T\phi(x_j)$를 커널로 대체하고 최적화 문제를 풀어 w를 구한다.
그리고 원래의 문제식에 넣으면 classifier를 얻을 수 있다.

$$
\begin{aligned}
  y(x) &= sign(w^T\phi(x) + b) \quad (x : new) \\
  &= sign(\sum_{k=1}^{n}{\alpha_k y_k K(x, x_k) + b})
\end{aligned}
$$

논문에서는 $x$의 decision value인 $\sum_{k=1}^{n}{\alpha_k y_k K(x, x_k)}$를 신용점수로 사용한다.

<br>

# fuzzy SVM

standard SVM을 적용할 때 발생하는 주요 문제 중 하나는 training data의 과적합으로 인해 이상치나 noise에 민감하다는 것이다. <br>
속한 class에서 멀리 떨어져 있는 이상치가 있다면 평면이 많이 기울어진다고 생각하면 된다. <br>
fuzzy SVM은 이 문제를 해결하기 위해 제안된 방법론이다. <br>

> Unilateral-Weighted fuzzy SVM

개체가 속한 class로부터의 거리를 membership으로 정의하자. <br>
주요 아이디어는 오차항에 균등가중치가 아닌 적절한 membership에 따른 가중치를 주자는 것이다. <br>
이상치에 low-membership을 주면 오차항에 대한 영향력이 감소된다. <br>
덜 중요한 이상치의 민감도를 줄여 penalty term을 모호하게 만든다는 뜻에서 fuzzy SVM 이라고 한다.
최적화 문제는 다음과 같이 공식화할 수 있다.

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{m_k\xi_k}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

<br>

# new fuzzy SVM

기존에 제안되었던 fuzzy SVM 에서와 동일하게 이상치에 대해 낮은 membership을 주는 것은 동일하지만, 반대 class에 높은 membership을 가지는 관측치로 처리하자는 아이디어가 추가되었다. <br>
이는 목적함수의 error term에 관측치 당 2개의 오류값이 기여하는 방식이 된다. <br>
데이터를 최대한 활용하게 되므로 더 좋은 generalization abllity를 가질 것으로 기대할 수 잆다.
이전의 fuzzy SVM은 개체가 속한 class에 대한 방향, 일방향 가중치를 주었다는 의미에서 Unilateral-Weighted fuzzy SVM (U-FSVM) 이라고 하고, new fuzzy SVM은 양방향 가중치를 주었다는 의미에서 Bilateral-Weighted fuzzy SVM (B-FSVM) 이라고 한다.

<br>

> Bilateral-Weighted fuzzy SVM

신용평가에서 일반적으로 우수한 고객이나 그 반대로 분류하는 것은 불가능하다. <br>
우수고객이 상황에 따라 나중에 채무불이행을 할 수도 있는 것이다.
이러한 직관을 기반으로 각 개체를 good / bad class로 나누긴 하지만 membership은 두 집단에 대한 membership을 각각 할당한다. <br>
경제적인 의미로는 고객을 우수고객/잠재적 채무불이행 고객으로 취급한다는 것이다. <br>

이 방법을 적용하면 데이터는 2n개로 늘어나게 된다.

$$
(x_k, y_k) \quad \textrm{for} \; k=1,...,n\\
to \\
(x_k, 1, m_k), (x_k, -1, 1-m_k) \quad \textrm{for} \; k=1,...,n\\
$$

$m_k$는 $x_k$의 class $y_k$에 대한 membership을 의미한다.
최적화 문제는 다음과 같이 공식화 할 수 있다.

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k,\eta_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{[m_k\xi_k+(1-m_k)\eta_k]}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
 \quad & y_k(w^T\phi(x_k)+b) \leq-1+\eta_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
  &\eta_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

추가된 데이터에 대한 제약식도 추가되었다. <br>


> Formulation

$$
\begin{aligned}
\max_{\alpha_k,\beta_K,\mu_k, \nu_k}\min_{w,b,\xi_k,\eta_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{m_k\xi_k}+C\sum_{k=1}^{n}{(1-m_k)\eta_k} \\
& -\sum_{k=1}^{n}{\alpha_k(w^T\phi(x_k)+b-1+\xi_k)}+\sum_{k=1}^{n}{\beta_k(w^T\phi(x_k)+b+1-\eta_k)} \\
& -\sum_{k=1}^{n}{\mu_k\xi_k}-\sum_{k=1}^{n}{\nu_k\eta_k}
\end{aligned}
$$

라그랑지 방법으로 convex optimization 문제를 해결한다. <br>
목적함수, 부등제약함수는 convex 형태로 들어가야 하므로 - term으로 추가해야 한다. <br>
목적함수를 간단하게 $J$라고 하자.

<br>

1. lagrange primal
$$
\frac{\partial J}{\partial w}
$$


2. lagrange dual


<br>

# Comment
> 의의


> 장점 


> 단점

데이터 활용도가 높지만 과적합에 대한 우려가 있을 수 있다. <br>
저자도 이를 고려한 것인지 다른 기본적인 모델을 이용한 초기 신용점수를 구하고, memvership을 할당한 뒤 fuzzy SVM을 적용시켜 모델을 개선하는 형태의 방법론을 제시했다. <br>
