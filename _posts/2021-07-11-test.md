---
title: "A New Fuzzy Support Vector Machine to Evaluate Credit Risk"
excerpt: test subtitle
toc: true
toc_sticky: true

author_profile: false

date: 2021-08-20 19:40:00 -0000
categories: 
  - Credit Scoring
tags:
  - Credit Risk Evaluation
---

2021.08.20 세미나 발표 논문 정리. 

<https://ieeexplore.ieee.org/abstract/document/1556587>

# Introduction
은행, 신용보조기관은 신용 리스크를 관리하는 것이 매우 중요하다.<br>
돈을 빌려주고 받는 이자로 수익을 내는 것도 중요하지만, 고객의 파산, 채무불이행 리스크도 고려해야 한다.<br>
무엇보다 세심하게 평가되지 않은 신용등급, 신용점수로 인해 고객의 대출승인심사가 거절되는 등의 피해는 없어야 한다.


신용평가 영역에서는 'good creditor', 'bad creditor'을 절대적으로 나눌 수 없다. <br>
이러한 직관을 반영하여 단순히 class를 나누는 것이 아니라 membership(class에 속할 가능성, 점수와 같은 것)을 부여해서 'good'과 'bad' 일 가능성을 열어두면 좀 더 실용적인 모델이 될 것이라 기대할 수 있다.



# SVM
SVM은 기본적으로 class를 분류하는 문제이다. <br>
우리가 접하는 데이터는 대게 깔끔하게 class를 분리할 수 없다. <br>
original space에서는 분리할 수 없을 때, 고차원으로 mapping 해주고 직선이 아닌 평면으로 분리해주자는 아이디어를 생각하자. <br>
이 때 선형으로 분리가능한 hyperplane을 찾는 것이 목적이다. <br>
데이터와 hyperplane의 거리를 최대화하면 최적의 hyperplane을 구할 수 있다. <br>
각 class에서 가장 가까운 관측치 들 간의 거리를 margin이라 하고, 초평면 & 반공간에 대한 분류식에 의해 SVM은 margin을 최대화하는 convex 문제가 된다. <br>

<br>

> Hard Margin
> 
다음과 같은 training sample을 고려해보자. <br>

$$ (x_k, y_k) , \quad  k=1,..,n \\
where \quad x_k \in R^d, \quad  y_k \in (1,-1) $$

$x_k$ 는 고객의 특성이라고 볼 수 있고, $\,y_k$는 대출승인 여부, 채무불이행 여부 등이 될 수 있다. <br>
training sample은 비선형함수 $\phi(\bullet)$ 을 이용해 고차원으로 mapping 했을 때 선형으로 분리가능하다고 가정한다. <br>
분류기는 다음과 같이 표현된다.

$$
\begin{cases}
    w^T\phi(x) + b \geq 1  & \text{if }  y_k=1 \\
    w^T\phi(x) + b \leq -1, & \text{if }  y_k=-1 \\
\end{cases}
$$

 $x^+$을 + plane 위의 점, $x^-$ 을 - plane 위의 점이라고 하자. <br>
두 점은 $x^+=x^- + \lambda w$ 의 평행이동 관계가 있으므로 plane 간의 거리는 $2 / w^Tw$ 가 된다.

$$
w^Tx^+ b = 1 \\
\begin{aligned}
  \Rightarrow w^T(x^- + \lambda w) + b = 1 \\
  \Rightarrow w^Tx^- + b + \lambda w^Tw = 1 \\
  \Rightarrow -1 + \lambda w^Tw = 1 \\
  \Rightarrow \lambda = \frac{2}{w^Tw} 
\end{aligned}
$$


이 식을 이용해서 Margin을 구하면 $2 / ||w||_2$ 가 된다.


$$
\begin{aligned}
margin &= distance(x^+, x^-) \\
&= ||x^+- x^-||_2 \\
&= || (x^- + \lambda w) - x^-||_2 \\
&= || \lambda w||_2 \\
&= \lambda \sqrt{w^Tw} \\
&= \frac{2}{w^Tw} \sqrt{w^Tw} \\
&= \frac{2}{\sqrt{w^Tw}} = \frac{2}{||w||_2}

\end{aligned}


$$

목적함수는 이 margin을 최대화하는 것이고 1/margin을 최소화하는 문제와 같아진다. <br>
계산의 편의를 위해 일대일함수인 제곱형태로 수정해준 목적함수를 사용한다.

$$
min \, J(w,b) = min \, \frac{1}{2} ||w||_2^2
$$

<br>

> soft margin

살제 데이터는 고차원으로 mapping 해도 선형분리가 되지 않는 경우가 많다. <br>
즉, 데이터를 완전히 분리하는  hyperplane을 찾을 수 없다. <br>
약간의 오류를 허용하기 위해 error term $\xi_k$ 를 도입하면 다음과 같은 최적화 문제가 된다. <br>

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{\xi_k}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}

$$

<br>

정규화 모수 C는 2가지 목표를 trade-off 하는 상수이다. <br>
이상치 존재 가능성을 낮게 봐서 error term을 최소화하는데 집중하는 경우 large C를 사용하고, <br>
이상치 존재 가능성을 크게 봐서 margin을 최대화하고 싶은 경우 small C를 사용한다.


# Unilateral-Weighted fuzzy SVM
standard SVM을 적용할 때 발생하는 주요 문제 중 하나는 training data의 과적합으로 인해 이상치나 noise에 민감하다는 것이다.
fuzzy SVM은 이 문제를 해결하기 위해 제안된 방법론이다.
주요 아이디어는 오차항에 균등가중치가 아닌 membership을 가중치를 주자는 것이다.
membership은 개체가 속한 class로부터의 거리로 생각할 수 있다.


# Bilateral-Weighted fuzzy SVM