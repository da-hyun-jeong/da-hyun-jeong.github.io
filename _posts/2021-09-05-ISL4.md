---
title: Chap4. 분류
excerpt: ISL 
toc: true
toc_sticky: true

author_profile: false

date: 2021-09-05 21:30:00 -0000
categories: 
  - ISL 
tags:
  - Machine Learning
---

<br>

# 1. 분류의 개요 

질적 반응변수를 예측하는 것은 관측치를 분류하는 것이다. <br>
이 떄 사용되는 분류기는 로지스틱 회귀, 선형판별분석, K-최근접이웃이 있다. <br>
일반화가법모델, 트리, 랜덤포레스트 및 부스팅, SVM은 뒤에서 다룬다.

<br>
<br>

# 2. 왜 선형회귀를 사용하지 않는가?

<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132210493-6b5bc95d-84da-46e6-ae5a-6d5e3263a140.png">
</p>


<br>

이진분류 문제라면 반응변수 $Y$가 0 또는 1의 값으로 둘 수 있다. <br>
이 때 선형모형을 적합하면 위의 사진의 왼쪽 패널처럼 [0,1] 범위를 벗어나게 된다. <br>
(이진분류에 선형회귀를 사용하면 선형판별분석 결과와 동일하다.) <br>

반응변수가 다범주인 경우도 있다. <br>

$$
Y =
\begin{cases}
    1 & \text{뇌졸중인 경우 }  \\
    2 & \text{약물 과다복용인 경우 }   \\
	3 & \text{간질발작인 경우} 
\end{cases}
$$

위의 세 가지 의료상태가 긴밀한 관계가 있다고 하자. <br>
이 질적변수의 순서정보를 포함하면서도 양적변수로 바꿀 수 있는 방법은 없다.

<br>
<br>

# 3. 로지스틱 회귀(Logistic Regression)

로지스틱 회귀는 반응변수 $Y$를 직접 모델링하지 않고 $Y$가 특정 범주에 속하는 **확률**을 모델링한다. <br>
$p(X) = \text{Pr}(Y=1 | X)$와 $X$의 관계를 로지스틱 함수를 이용하여 모델링하면 다음의 형태가 된다.

$$
\text{log} \left( \frac{p(X)}{1-p(X)} \right) = \beta_0 + \beta_1X + ... \beta_pX

$$

log-odds 또는 logit은 $X$에 대해 선형이다. <br>
$X$가 한 단위 증가할 때 로짓은 평균적으로 $e^{\beta_1}$만큼 증가한다고 해석한다. <br>
반응변수의 범주가 3개 이상일 때는 다범주 로지스틱 회귀보다 판별분석을 사용한다. 

<br>
<br>

# 4. 선형판별분석(Linear Discriminant Analysis)

로지스틱 회귀는 로지스틱 함수를 사용하여 반응변수 클래스에 대해 $\text{Pr}(Y=k|X=x)$를 직접 모델링한다. <br>
이러한 확률 추정에 대해 좀 더 간접적인 대안을 고려해보자. <br>
반응변수 $Y$의 각 클래스에서 설명변수 $X$의 분포를 모델링하고, 그 다음에 베이즈 정리를 사용해서 $\text{Pr}(Y=k|X=x)$에 대한 추정치를 얻는다. <br>
이 분포들이 정규분포라고 가정하면 모델은 로지스틱 회귀와 형태가 아주 비슷하다. 

<br>

로지스틱 회귀가 있는데 왜 다른 방법을 사용할까 ? <br>

- 클래스들이 잘 분리될 때 로지스틱 모델의 추정치는 아주 불안정하지만, 선형판별분석은 이런 문제가 없다. 
- n이 작고 각 클래스에서 설명변수 $X$의 분포가 근사적으로 정규분포이면, 선형판별모델은 로지스틱모델보다 안정적이다.
- 반응변수 클래스가 3 이상일 때 일반적으로 사용한다.

<br>



> 베이즈 정리

범주 $K$가 3개 이상인 명목형 변수를 분류하는 문제를 생각하자. 
- $\pi_k$ : 랜덤하게 선택된 관측치가 $k$번째 클래스에서 뽑혔을 확률 (사전확률)
- $f_k(X)$ = $\text{Pr}(X=x \mid Y=k)$ : $k$번째 클래스에 속하는 관측치에 대한 $X$의 확률밀도함수 

베이즈 정리를 사용하면 다음과 같다.

$$
p_k(X) = \text{Pr}(Y=k \mid X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

$Y$의 랜덤표본이 있으면 $\pi_k$는 단순히 $k$번째 클래스에 속할 훈련 관측치들의 비율로 쉽게 추정할 수 있다. <br>
$f_k(X)$는 밀도에 대한 형태를 가정해서 추정해야 한다. <br>
따라서 사후확률은 이 둘을 대입해서 구하면 된다. <br>
베이즈 분류기에 근접하는 분류기를 찾으려면 $f_k (X)$를 잘 추정하면 될 것이다.

<br>
정리하면, LDA 분류기는 각 클래스 내의 관측치들이 클래스 별 평균벡터와 공통분산을 갖는 정규분포를 따른다는 가정하에 이 파라미터들에 대한 추정값을 베이즈 분류기에 대입하여 얻는다.

<br>

> 선형판별분석 ( $p$=1 )

설명변수가 하나만 있다고 가정해보자. <br>
$f_k(x)$는 정규분포라고 하자.

$$
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k} exp \left(-\frac{1}{2\sigma^2_k} (x-\mu_k)^2 \right)
$$

각 클래스의 분산은 같다고 가정하고 $\sigma^2$라고 표기한다. <br>
이 밀도함수로 사후확률 $p_k(x)$를 구하면 다음을 얻는다.

$$
p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma} exp \left(-\frac{1}{2\sigma^2} (x-\mu_k)^2 \right)} {\sum_{l=1} ^{K} \pi_l \frac{1}{\sqrt{2\pi}\sigma} exp \left(-\frac{1}{2\sigma^2} (x-\mu_l)^2 \right)}
$$

<br>

베이즈 분류기는 $p_k(x)$가 최대가 되는 클래스에 관측치 $X=x$를 할당한다. <br>
$p_k(x)$에 로그를 취해 정리하면 아래의 식이 얻어진다. <br>
베이즈 분류기는 이 식을 최대로 하는 클래스에 관측치를 할당하는 것과 동일하다.
<br>

$$
\delta_k(x)= x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \text{log} (\pi_k) 
$$

<br>

예시를 통해 분류기와 분류 경계를 구하는 과정을 이해해보자. <br>
$K$=2이고, $\pi_1 =\pi_2$인 간단한 케이스를 고려해보자. <br>
분류기의 경계는 $\delta_1=\delta_2$로 둘 수 있으므로 $\delta_1 > \delta_2$ 이면 1이고, 그렇지 않으면 2라고 하자. <br>
구체적으로는 다음과 같다.

$$
x \cdot \frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2} + \text{log} (\pi_1) > x \cdot \frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2} + \text{log} (\pi_2) \\
$$

$$
\begin{aligned}
  &\Rightarrow 2x(\mu_1 -\mu_2) > \mu_1^2  - \mu_2^2 \quad \\
  &\Rightarrow x > \frac{\mu_1^2  - \mu_2^2}{2(\mu_1 -\mu_2)} = \frac{\mu_1+\mu_2}{2} \quad (decision \; rule) 

\end{aligned}
$$

<br>


<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132210340-887c5dd9-799c-4a9f-8c0a-0b1d5ce2c50a.png">
</p>


<br>

구체적으로 두 밀도함수 $f_1(x)$와 $f_2(x)$가 각각 $\mu_1$=-1.25, $\mu_2$=1.25, $\sigma_1=\sigma_2$=1 을 갖는다고 하자. <br>
아까 구한 결정경계에  $\pi_1 =\pi_2$=0.5를 대입했을 때, 관측치가 $x$>0이면 2로 $x$<0이면 클래스 1로 할당한다.
<br>

그러나 실제로는 파라미터를 모르기 때문에 추정해야 한다.

$$
\hat{\mu}_k = \frac{1}{n_k} \sum_{i:y_i=k} x_i 
$$

$$
\hat{\sigma}^2 = \frac{1}{n-K} \sum_{k=1}^{K} \sum_{i:y_i=k} (x_i-\hat{\mu}_k)^2 
$$

$$
\hat{\pi}_k = \frac{n_k}{n}
$$
 
<br>

이제 이 추정치들을 대입해서 LDA 분류기를 구한다. 

$$
\hat{\delta}_k(x)= x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + \text{log} (\hat{\pi}_k) 
$$

분류기 이름에 선형이라는 말이 있는 것은 $\hat{\delta}_k(x)$가 $x$의 선형함수이기 때문이다.

<br>

<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132210330-9636d923-e5cc-4f2c-8973-85331647c7f0.png">
</p>


<br>

각 클래스에 랜덤으로 뽑은 20개의 관측치의 히스토그램이다. <br>
실제 결정경계는 점선, 추정된 결정경계는 실선으로 나타내었다. <br>
$n_1=n_2$ = 20 이므로 $\hat{\pi_1}=\hat{\pi_2}$이다. <br>
따라서 결정경계는 표본평균들 사이의 중간이 된다.

<br>


> 선형판별분석 ( $p$>1 )

설명변수가 $p$개인 경우 LDA 분류기는 $k$번째 클래스의 관측치들이 다변량정규분포 $N(\mu_k, \Sigma)$를 따른다고 가정한다. <br>
즉 $X = (X_1, X_2, ..., X_p)$는 클래스에 따라 특정 평균벡터 $E(X)=\mu$와 공통된 공분산행렬 $\Sigma$ 를 가지는 다변량정규분포를 따른다. <br>

$$
f(x)=\frac{1}{(2\pi)^p |\Sigma|^{1/2}}exp\left(-\frac{1}{2} (x-\mu)^T \Sigma ^{-1} (x-\mu) \right)
$$


이 밀도함수를 사후확률 $p_k(X)$애 대입하여 정리하면 베이즈 분류기는 관측치 $x$를 다음 식이 최대가 되는 클래스에 할당한다.

$$
\delta_k = x^T \Sigma^{-1}\mu_k - \frac{1}{2} \mu_k^T\Sigma^{-1}\mu_k + \text{log} (\pi_k)
$$

<br>

<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132210920-c29fef93-279b-4305-80fa-994530cf8cc7.png">
</p>



<br>

세 개의 크기가 같은 클래스가 각각의 평균벡터와 공통의 공분산행렬을 가진다고 하자. <br>
타원은 관측치들의 95%를 포함하는 영역을 나타낸다. <br>
점선은 베이즈 결정경계를 나타내고, 이 경계들은 $k\neq l$ 에 대해 $\delta_k(x)=\delta_l(x)$를 만족하는 $x$값의 집합을 나타낸다. <br>
여기서 클래스 크기가 같으므로 $\text{log}(\pi_k)$ 항은 소거된다.

$$
x^T \Sigma^{-1}\mu_k - \frac{1}{2} \mu_k^T\Sigma^{-1}\mu_k  = x^T \Sigma^{-1}\mu_l - \frac{1}{2} \mu_l^T\Sigma^{-1}\mu_l 
$$

세 개의 결정경계가 생기므로 설명변수 공간을 세 개의 영역으로 나눌 수 있다. <br>


<br>

<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132210337-ccce66a5-6709-4175-8a9b-76e35864d1a2.png">
</p>


<br>

실제로는 아까와 같이 파라미터를 추정해서 구한다.

<br>

> 이차선형판별분석(Quadratic Discriminant Analysis)

QDA는 LDA처럼 각 클래스의 관측치들이 정규분포를 따른다고 가정하고, 파라미터들에 대한 추정치를 베이즈 정리에 대입하여 예측을 한다. <br>
하지만 LDA와 달리 가정을 조금 완화하여 각 클래스가 자체 공분산행렬을 갖는다고 가정한다. <br>
즉, $k$번째 클래스의 관측치는 $X \sim N(\mu_k, \Sigma_k)$ 형태라고 가정한다. <br>
이러한 가정 하에서 베이즈 분류기는 관측치 $X=x$를 다음 식이 최대가 되는 클래스에 할당한다.

$$
\begin{aligned}
\delta_k(x) &= -\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1}(x-\mu_k) -\frac{1}{2}
\text{log}|\Sigma_k| + \text{log} \pi_k \\
&=  -\frac{1}{2}x^T \Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k -\frac{1}{2}\mu_k^T \Sigma_k^{-1}\mu_k -\frac{1}{2}\text{log}|\Sigma_k| + \text{log} \pi_k 
\end{aligned}
$$

따라서 QDA 분류기는 $\Sigma_k, \, \mu_k, \, \pi_k$에 대한 추정치를 대입해서 이 식이 최대가 되는 클래스에 관측치 $X=x$를 할당한다. 

<br>

**$K$개의 클래스에 대해 공분산 가정을 하는 것이 왜 중요할까 ?** <br>
이는 $bias-variance \; trade-off$ 때문이다. <br>
설명변수가 $p$개일 때, LDA는 하나의 공분산 행렬을 추정하므로 $p(p+1)/2$개의 파라미터를 추정해야 하고, QDA는 $Kp(p+1)/2$개의 파라미터를 추정해야 한다. <br>
설명변수 갯수가 커질수록 이 차이는 커지게 된다. <br>
LDA는 파라미터 갯수가 적기 때문에 QDA보다 유연성이 떨어지지만 낮은 분산을 가지며, 높은 편향을 갖는다. <br>
훈련 데이터 크기가 작아 분산을 줄이는 것이 중요하다면 LDA를 사용하고, 훈련 데이터 크기가 커서 분류기의 분산이 주요 고려사항이 아니라면 QDA가 권장된다.

<br>
<br>

# 5. 분류방법의 비교

이 때까지 살펴보았던 분류방법 : KNN, 로지스틱 회귀, LDA, QDA를 비교한다. 

<br>

> 로지스틱 회귀와 LDA

설명변수 갯수가 하나인 binary setting 에서 $p_1(x)$와 $p_2(x)=1-p_1(x)$는 관측치 $X=x$가 클래스 1과 2에 속할 확률이라고 하자. <br>
LDA 관점에서 log-odds는 다음과 같이 주어진다.

$$
\text{log} \left(\frac{p_1(x)}{1-p_1(x)} \right) = c_0 + c_1x
$$

로지스틱 회귀는 다음과 같이 표현된다.

$$
\text{log} \left(\frac{p_1}{1-p_1} \right) = \beta_0 + \beta_1x
$$

<br>

위 두 식은 모두 $x$의 선형함수이므로 선형 결정경계를 만든다. <br>
두 기법 사이의 차이점은 로지스틱은 최대가능도를 사용하여 추정하고, LDA는 정규분포로 추정된 평균과 분산을 사용한다는 것이다. <br>
정규분포를 따른다는 가정이 잘 맞으면 LDA가 로지스틱 회귀보다 더 나은 결과를 제공하고, 반대로 가우스 가정이 만족되지 않으면 로지스틱 회귀가 LDA보다 더 나은 성능을 낼 수 있다.

<br>

> KNN

KNN은 관측치 $X=x$에 대해 예측하기 위해 $x$에 가장 가까운 $K$개의 훈련 관측치를 식별하고, 이 관측치들이 속하는 클래스에 할당된다. <br>
따라서 KNN은 완전히 비모수적인 방법이므로 결정경계의 형태에 대해 어떠한 가정도 하지 않는다. <br>
이 기법은 결정경계가 비선형일 때 로지스틱 회귀와 LDA보다 우수한 성능을 가진다. <br>
반면에 비모수적인 방법이므로 계수를 얻을 수 없기 때문에 어떤 설명변수가 중요한지는 알 수 없다. 

<br>

> QDA

QDA는 비모수적인 방법인 KNN과 선형의 LDA 및 로지스틱 회귀 사이에서 절충한 것이다.
QDA는 이차 결정경계를 가정하므로 선형방법들보다 넓은 범위의 문제들을 정확하게 모델링 할 수 있다. <br>
KNN 만큼 유연하지는 않지만 결정경계의 형태에 대해 몇 가지 가정을 하기 때문에 훈련데이터의 수가 제한적일 때 KNN보다 더 나은 성능을 낼 수 있다.

<br>

> 마무리

모든 상황에서 다른 것보다 월등히 나은 방법은 없다. <br>
결정경계가 실제로 선형일 때 LDA와 로지스틱 회귀 기법이 좋은 성능을 내고, 경계가 적당히 비선형적일 때는 QDA가 더 좋은 결과를 낼 수 있다. <br>
훨씬 더 복잡한 비선형 결정경계의 경우 KNN과 같은 비모수 방법이 더 나을 수 있다. <br>
하지만 비모수방법은 평활 수준을 주의깊게 선택해야 한다.
<br>

선형회귀 분석에서 처럼 $X^2, X^3$ 항을 설명변수로 포함하면 보다 유연한 버전의 로지스틱 회귀를 만들 수 있다. <br>
LDA에 대해서도 동일한 방식이 적용될 수 있는데, 모든 가능한 이차항과 cross-product 항을 LDA에 추가하면 추정치는 다르지만 모델은 QDA와 같아진다.






































