---
title: "A New Fuzzy Support Vector Machine to Evaluate Credit Risk(2005)"
excerpt: fuzzy SVM to credit scoring
toc: true
toc_sticky: true

author_profile: false

date: 2021-08-20 19:40:00 -0000
categories: 
  - Credit Risk
  - paper review
tags:
  - Credit Risk Evaluation
  - Machine Learning
---

<br>

2021.08.20 세미나 발표 논문 정리. <br>
standard SVM의 이상치에 민감하다는 단점을 보완한 fuzzy SVM 방법론에 관한 논문입니다. <br>
오타가 많으니 직접 읽어보실 분들은 주의하시길 바랍니다 !
<https://ieeexplore.ieee.org/abstract/document/1556587>

<br>

# Introduction
은행, 신용보조기관은 신용 리스크를 관리하는 것이 매우 중요하다.<br>
돈을 빌려주고 받는 이자로 수익을 내는 것도 중요하지만, 고객의 파산, 채무불이행 리스크도 고려해야 한다.<br>
무엇보다 세심하게 평가되지 않은 신용등급, 신용점수로 인해 고객의 대출승인심사가 거절되는 등의 피해는 없어야 한다.


신용평가 영역에서는 'good creditor', 'bad creditor'을 절대적으로 나눌 수 없다. <br>
이러한 직관을 반영하여 단순히 class를 나누는 것이 아니라 membership(class에 속할 가능성, 점수와 같은 것)을 부여해서 'good'과 'bad' 일 가능성을 열어두면 좀 더 실용적인 모델이 될 것이라 기대할 수 있다.

<br>

# SVM
SVM은 기본적으로 class를 분류하는 문제이다. <br>
우리가 접하는 데이터는 대게 깔끔하게 class를 분리할 수 없다. <br>
original space에서는 분리할 수 없을 때, 고차원으로 mapping 해주고 직선이 아닌 평면으로 분리해주자는 아이디어를 생각하자. <br>
이 때 선형으로 분리가능한 hyperplane을 찾는 것이 목적이다. <br>
데이터와 hyperplane의 거리를 최대화하면 최적의 hyperplane을 구할 수 있다. <br>
각 class에서 가장 가까운 관측치들 간의 거리를 margin이라 하고, 초평면 & 반공간에 대한 분류식에 의해 SVM은 margin을 최대화하는 convex 문제가 된다. <br>

<br>

> Hard Margin

다음과 같은 training sample을 고려해보자. <br>

$$ (x_k, y_k) , \quad  \textrm{for} \; k=1,...,n \\
where \quad x_k \in R^d, \quad  y_k \in (1,-1) $$

$x_k$ 는 고객의 특성이라고 볼 수 있고, $\,y_k$는 대출승인 여부, 채무불이행 여부 등이 될 수 있다. <br>
training sample은 비선형함수 $\phi(\bullet)$ 을 이용해 고차원으로 mapping 했을 때 선형으로 분리가능하다고 가정한다. <br>

<br>


  1. 분류기 식 구하기

고차원 facture space에서 구분 가능한 초평면은 다음과 같다.

$$ 
 w^T\phi(x) + b = 0
$$

$x^+$을 + plane 위의 점, $x^-$ 을 - plane 위의 점이라고 하자. <br>
주어진 초평면과 같은 법선벡터를 가지며 $x^+$, $x^-$를 지나는 초평면은 다음과 같이 표현된다.

$$
\begin{cases}
    w^T\phi(x) + b = 1  & \text{if }  y_k=1 \\
    w^T\phi(x) + b = -1, & \text{if }  y_k=-1 \\
\end{cases}
$$

초평면 사이에는 데이터가 존재하면 안되므로 공간에 제약을 준다. <br>
즉, 분류기는 다음과 같이 표현된다.

$$
\begin{cases}
    w^T\phi(x) + b \geq 1  & \text{if }  y_k=1 \\
    w^T\phi(x) + b \leq -1, & \text{if }  y_k=-1 \\
\end{cases}
$$

위의 두 식은 아래와 같이 하나의 식으로 나타낼 수 있다. <br>
(이 식은 최적화 과정에서 제약조건으로 사용된다.)

$$
y_k(w^T\phi(x_k)+b) \geq1 ,  \quad \textrm{for} \; k=1,...,n\\
$$

<br>

  2. margin 최대화 해주기

초평면을 정의하면 margin을 최대화하는 과정이 따라온다. <br>
분류기 식을 유도했던 과정을 생각해보면 초평면 간에는 평행이동 관계가 있다. <br>
즉, $x^+=x^- + \lambda w$의 평행이동 관계가 있으므로 plane 간의 거리는 $2/w^Tw$가 된다.

$$
w^Tx^+ b = 1 \\
\begin{aligned}
  \Rightarrow w^T(x^- + \lambda w) + b = 1 \\
  \Rightarrow w^Tx^- + b + \lambda w^Tw = 1 \\
  \Rightarrow -1 + \lambda w^Tw = 1 \\
  \Rightarrow \lambda = \frac{2}{w^Tw} 
\end{aligned}
$$


이 식을 이용해서 margin을 구하면 $2 / \mid\mid w \mid\mid_2$가 된다.


$$
\begin{aligned}
margin &= distance(x^+, x^-) \\
&= ||x^+- x^-||_2 \\
&= || (x^- + \lambda w) - x^-||_2 \\
&= || \lambda w||_2 \\
&= \lambda \sqrt{w^Tw} \\
&= \frac{2}{w^Tw} \sqrt{w^Tw} \\
&= \frac{2}{\sqrt{w^Tw}} = \frac{2}{||w||_2}

\end{aligned}


$$

결론적으로 최적화 문제에서 목적함수는 이 margin을 최대화하는 것이고 1/margin을 최소화하는 문제와 같아진다. <br>
계산의 편의를 위해 제곱형태로 수정해준 목적함수를 사용한다.

$$
min \, J(w,b) = min \, \frac{1}{2} ||w||_2^2
$$

<br>

> soft margin

살제 데이터는 고차원으로 mapping 해도 선형분리가 되지 않는 경우가 많다. <br>
즉, 데이터를 완전히 분리하는  hyperplane을 찾을 수 없다. <br>
약간의 오류를 허용하기 위해 error term $\xi_k$ 를 도입하면 다음과 같은 최적화 문제가 된다. <br>

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{\xi_k}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

<br>

정규화 모수 C는 2가지 목표를 trade-off 하는 상수이다. <br>
이상치 존재 가능성을 낮게 봐서 error term을 최소화하는데 집중하는 경우 large C를 사용하고, <br>
이상치 존재 가능성을 크게 봐서 margin을 최대화하고 싶은 경우 small C를 사용한다.

<br>

> kernel SVM

$\phi(x)$는 input vector를 고차원으로 mapping 해주는 함수이다. <br>
그 차원에서 두 개의 class가 선형분리된다. <br>
실제로는 QP solution을 구하는 데 필요한 $\phi(x)$의 정확한 형태를 알기 어렵다. <br>
kernel SVM은 이를 해결하기 위해 facture space에서 내적 형태인 kernel function을 정의하고, 이 함수로 feature space에서 데이터를 선형으로 분리한다. <br>
따라서 $\phi(x)$를 몰라도 최적의 mapping 함수를 찾을 수 있다.

$$
K(x_i , x_j) = \phi(x_i)^T\phi(x_j)
$$

커널이 Mercer의 조건을 만족하면 위와 같은 형태의 $\phi(x)$가 존재한다. <br>
$\phi(x_i)^T\phi(x_j)$를 커널로 대체하고 최적화 문제를 풀어 w를 구한다. <br>
그리고 원래의 문제식에 넣으면 classifier를 얻을 수 있다.

$$
\begin{aligned}
  y(x) &= sign(w^T\phi(x) + b) \quad (x : new) \\
  &= sign(\sum_{k=1}^{n}{\alpha_k  K(x, x_k) + b})
\end{aligned}
$$

논문에서는 $x$의 decision value인 $\sum_{k=1}^{n}{\alpha_k K(x, x_k)}$를 신용점수로 사용한다.

<br>

# fuzzy SVM

standard SVM을 적용할 때 발생하는 주요 문제 중 하나는 training data의 과적합으로 인해 이상치나 noise에 민감하다는 것이다. <br>
속한 class에서 멀리 떨어져 있는 이상치가 있다면 평면이 많이 기울어진다고 생각하면 된다. <br>
fuzzy SVM은 이 문제를 해결하기 위해 제안된 방법론이다. <br>

> Unilateral-Weighted fuzzy SVM

개체가 속한 class로부터의 거리를 membership으로 정의하자. <br>
주요 아이디어는 오차항에 균등가중치가 아닌 적절한 membership에 따른 가중치를 주자는 것이다. <br>
이상치에 low-membership을 주면 오차항에 대한 영향력이 감소된다. <br>
덜 중요한 이상치의 민감도를 줄여 penalty term을 모호하게 만든다는 뜻에서 fuzzy SVM 이라고 한다. <br>
최적화 문제는 다음과 같이 공식화할 수 있다.

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{m_k\xi_k}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

<br>

# new fuzzy SVM

기존에 제안되었던 fuzzy SVM 에서와 같이 이상치에 대해 낮은 membership을 주는 것은 동일하지만, 동시에 반대 class에 높은 membership을 가지는 관측치로 처리하자는 아이디어가 추가되었다. <br>
이는 목적함수의 error term에 관측치 당 2개의 오류값이 기여하는 방식이 된다. <br>
데이터를 최대한 활용하게 되므로 더 좋은 generalization abllity를 가질 것으로 기대할 수 있다. <br>
이전의 fuzzy SVM은 개체가 속한 class에 대한 방향, 일방향 가중치를 주었다는 의미에서 Unilateral-Weighted fuzzy SVM (U-FSVM) 이라고 하고, new fuzzy SVM은 양방향 가중치를 주었다는 의미에서 Bilateral-Weighted fuzzy SVM (B-FSVM) 이라고 한다.

> Bilateral-Weighted fuzzy SVM

신용평가에서 일반적으로 절대적인 우수한 고객이나 그 반대로 분류하는 것은 불가능하다. <br>
우수고객이 상황에 따라 나중에 채무불이행을 할 수도 있는 것이다. <br>
이러한 직관을 기반으로 각 개체를 good / bad class로 나누긴 하지만 membership은 두 집단에 대한 membership을 각각 할당한다. <br>
경제적인 의미로는 고객을 잠재적 우수고객/채무불이행 고객으로 취급한다는 것이다. <br>

이 방법을 적용하면 데이터는 2n개로 늘어나게 된다.

$$
(x_k, y_k) \quad \textrm{for} \; k=1,...,n\\
to \\
(x_k, 1, m_k), (x_k, -1, 1-m_k) \quad \textrm{for} \; k=1,...,n\\
$$

$m_k$는 $x_k$의 class $y_k$에 대한 membership을 의미한다. <br>
최적화 문제는 다음과 같이 공식화 할 수 있다.

$$
\begin{aligned}
\textrm{minimize}_{w,b,\xi_k,\eta_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{[m_k\xi_k+(1-m_k)\eta_k]}\\
\textrm{subject to} \quad & y_k(w^T\phi(x_k)+b) \geq1-\xi_k ,  \quad \textrm{for} \; k=1,...,n\\
 \quad & y_k(w^T\phi(x_k)+b) \leq-1+\eta_k ,  \quad \textrm{for} \; k=1,...,n\\
  &\xi_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
  &\eta_k\geq0  ,  \quad \textrm{for} \; k=1,...,n  \\
\end{aligned}
$$

추가된 데이터에 대한 제약식도 추가되었다. <br>

<br>

> Formulation

$$
\begin{aligned}
\max_{\alpha_k,\beta_K,\mu_k, \nu_k}\min_{w,b,\xi_k,\eta_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{m_k\xi_k}+C\sum_{k=1}^{n}{(1-m_k)\eta_k} \\
& -\sum_{k=1}^{n}{\alpha_k(w^T\phi(x_k)+b-1+\xi_k)}+\sum_{k=1}^{n}{\beta_k(w^T\phi(x_k)+b+1-\eta_k)} \\
& -\sum_{k=1}^{n}{\mu_k\xi_k}-\sum_{k=1}^{n}{\nu_k\eta_k}
\end{aligned}
$$

라그랑지 방법으로 convex optimization 문제를 해결한다. <br>
목적함수, 부등제약함수는 convex 형태로 들어가야 하므로 - term으로 추가되는 항도 있으니 주의하자. <br>
목적함수를 간단하게 $J$라고 하자.

<br>

1. lagrange primal

$$
\begin{aligned}
\min_{w,b,\xi_k,\eta_k} \quad & \frac{1}{2}w^{T}w+C\sum_{k=1}^{n}{m_k\xi_k}+C\sum_{k=1}^{n}{(1-m_k)\eta_k} \\
& -\sum_{k=1}^{n}{\alpha_k(w^T\phi(x_k)+b-1+\xi_k)}+\sum_{k=1}^{n}{\beta_k(w^T\phi(x_k)+b+1-\eta_k)} \\
& -\sum_{k=1}^{n}{\mu_k\xi_k}-\sum_{k=1}^{n}{\nu_k\eta_k}
\end{aligned}
$$

위의 라그랑지 프리멀 함수의 최솟값을 구하는 과정이다.

$$
\begin{aligned}
& \frac{\partial J}{\partial w}=w-\sum_{k=1}^{n}{\alpha_k\phi(x_k)}+\sum_{k=1}^{n}{\beta_k\phi(x_k)}=0 \\
& \frac{\partial J}{\partial b}=-\sum_{k=1}^{n}{\alpha_k}+\sum_{k=1}^{n}{\beta_k}=0 \\
& \frac{\partial J}{\partial \xi_k}=Cm_k-\alpha_k-\mu_k = 0 \quad \textrm{for} \; k=1,...,n\\
& \frac{\partial J}{\partial \eta_k}=C(1-m_k)-\beta_k-\nu_k \quad \textrm{for} \; k=1,...,n\\
\end{aligned}

$$


식을 정리해주면 다음과 같은 해를 구할 수 있다.

$$
w=\sum_{k=1}^{n}{(\alpha_k-\beta_k)\phi(x_k)} \\
\sum_{k=1}^{n}{\alpha_k}=\sum_{k=1}^{n}{\beta_k} \\
Cm_k=\alpha_k+\mu_k\quad \textrm{for} \; k=1,...,n\\
C(1-m_k)=\beta_k+\nu_k \quad \textrm{for} \; k=1,...,n\\
$$


- KKT condition

primal problem이 convex이면 KKT 조건을 이용해 동시에 primal, dual optimal value 임을 알 수 있다. <br>
stationarity, primal feasibility, dual feasibility, complementary slackness 라는 네 조건을 만족함을 보이면 된다. <br>
간단하므로 과정은 생략하고 구해진 조건들은 다음을 만족하게 된다.

$$
\alpha_k(w^T\phi(x_k)+b-1+\xi_k)=0 \\
\beta_k(w^T\phi(x_k)+b+1-\eta_k)=0 \\
\mu_k\xi_k=0 \\
\nu_k\eta_k=0 \\
\alpha_k \geq0, \; \beta_k \geq0, \; \mu_k \geq0, \; \nu_k \geq0, \; \xi_k \geq0, \; \eta_k \geq0
$$

<br>

- $\alpha_k$와 $\beta_k$의 범위

$$
\alpha_k \geq0, \; \mu_k \geq0, \; Cm_k=\alpha_k+\mu_k\quad \; \Rightarrow 0 \leq \alpha_k \leq Cm_k \\
\beta_k \geq0, \; \nu_k \geq0, \; C(1-m_k)=\beta_k+\nu_k\quad \; \Rightarrow 0 \leq \beta_k \leq C(1-m_k)
$$

<br>

- implict constraint

$$
Cm_k-\alpha_k=\mu_k, \quad  \mu_k\xi_k=0 \\
\alpha_k \geq0, \; \mu_k \geq0, \; \xi_k \geq0
$$

위의 조건들을 점검해보면  $\mu_k>0$이면 $\xi_k=0$ 이어야 하므로 동시에 $\mu_k>0 ,\;\mu_k\xi_k>0$ 은 불가능하다. <br>
둘 중 하나의 조건이 만족되지 않는 상황이 생기므로 제약식을 추가로 고려해야 한다.

$$ (Cm_k-\alpha_k)\xi_k=0 $$

마찬가지로 다음과 같은 조건들을 고려하면 

$$
C(1-m_k)-\beta_k=\nu_k, \quad  \nu_k\eta_k=0 \\
\beta_k \geq0, \; \nu_k \geq0, \; \eta_k \geq0
$$

두 번째 implict constraint를 얻을 수 있다. <br>


$$ (C(1-m_k)-\beta_k)\eta_k=0  $$


주의 깊게 보면 위 두 조건들은 목적함수의 2번째 term과 관련있음을 확인할 수 있다.

<br>

- 제약 조건 간소화 (통합)

dual form으로 바꿔주기 위해 KKT 조건의 complementary slackness를 확인하는 과정에서 구한 식들을 sum을 해서 빼주는 형태로 간소화한다.

$$
\sum_{k=1}^{n}{(\alpha_k-\beta_k)w^T\phi(x_k)} + b\sum_{k=1}^{n}{(\alpha_k-\beta_k)} \\
-\sum_{k=1}^{n}{(\alpha_k+\beta_k)}+\sum_{k=1}^{n}{(\alpha_k\xi_k+\beta_k\eta_k)}=0
$$

여기서 마지막으로 남은 조건인 $\sum_{k=1}^{n}{\alpha_k}=\sum_{k=1}^{n}{\beta_k}$를 대입하면 다음과 같이 정리된다.

$$
\sum_{k=1}^{n}{(\alpha_k-\beta_k)w^T\phi(x_k)} -\sum_{k=1}^{n}{(\alpha_k+\beta_k)}+\sum_{k=1}^{n}{(\alpha_k\xi_k+\beta_k\eta_k)}=0
$$

<br>

implict constraints에 마찬가지로 sum을 해주면 다음과 같다.

$$
\sum_{k=1}^{n}{(Cm_k-\alpha_k)\xi_k}=\sum_{k=1}^{n}{Cm_k\xi_k}-\sum_{k=1}^{n}{\alpha_k\xi_k}=0 \; \Rightarrow \; \sum_{k=1}^{n}{Cm_k\xi_k}=\sum_{k=1}^{n}{\alpha_k\xi_k} \\
\sum_{k=1}^{n}{(C(1-m_k)-\beta_k)\eta_k}=\sum_{k=1}^{n}{C(1-m_k)\eta_k}-\sum_{k=1}^{n}{\beta_k\eta_k}=0 \; \Rightarrow \; \sum_{k=1}^{n}{C(1-m_k)\eta_k}=\sum_{k=1}^{n}{\beta_k\eta_k}
$$

직전에 구한 식에 위의 식을 대입하면 조건들을 정리하는 과정은 끝이 난다. <br>
아래의 식은 목적함수의 2번째 term과 정확하게 같아진다.

$$
C\sum_{k=1}^{n}{m_k\xi_k}+C\sum_{k=1}^{n}{(1-m_k)\eta_k} \\
= \sum_{k=1}^{n}{(\alpha_k+\beta_k)}-\sum_{k=1}^{n}{(\alpha_k-\beta_k)w^T\phi(x_k)}
$$

드디어 마지막으로 미분하는 과정에서 구해진 $w$를 대입해주면 dual form이 완성된다. <br>
굉장히 길어보지만 직접 해보면 어렵지 않다 !


<br>

  2. lagrange dual

$$
\begin{aligned}
\max_{\alpha_k,\beta_K} \quad & \sum_{k=1}^{n}{(\alpha_k+\beta_k)}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}{(\alpha_i-\beta_i)(\alpha_j-\beta_j)\phi(x_i)^T\phi(x_j)} \\


\textrm{subject to} \quad &\sum_{j=1}^{n}{\alpha_k}=\sum_{j=1}^{n}{\beta_k}   \\
\quad & 0 \leq \alpha_k \leq Cm_k \quad \textrm{for} \; k=1,...,n \\
\quad & 0 \leq \beta_k \leq C(1-m_k) \quad \textrm{for} \; k=1,...,n
\end{aligned}
$$

여기서 $K(x_i, x_j)=\phi(x_i)^T\phi(x_j)$로 두고, QP form으로 변환하기 위해 $\gamma_k=\alpha_k-\beta_k$라 하자. <br>
최적화 문제를 다시 나타내면 최종적으로 다음의 식이 된다.

$$
\begin{aligned}
\max_{\beta_k,\gamma_k} \quad & \sum_{k=1}^{n}{\gamma_k}+2\sum_{k=1}^{n}{\beta_k}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}{\gamma_i\gamma_jK(x_i,x_j)} \\


\textrm{subject to} \quad &\sum_{k=1}^{n}{\gamma_k}=0  \\
\quad & 0 \leq \beta_k+\gamma_k \leq Cm_k \quad \textrm{for} \; k=1,...,n \\
\quad & 0 \leq \beta_k \leq C(1-m_k) \quad \textrm{for} \; k=1,...,n
\end{aligned}
$$

$\beta,\gamma$를 구하고 $w=\sum_{k=1}^{n}{(\alpha_k-\beta_k)\phi(x_k)}$를 원래의 분류 문제에 대입하면 classifier를 구할 수 있다.

$$
\begin{aligned}
  y(x) &= sign(w^T\phi(x) + b) \quad (x : new) \\
  &= sign(\sum_{k=1}^{n}{(\alpha_k-\beta_k)K(x, x_k) + b})
\end{aligned}
$$

decision value $\sum_{k=1}^{n}{(\alpha_k-\beta_k)K(x,x_k)}$를 신용점수로 사용한다.

<br>

> Generating Membership

이 방법론을 적용할 때 가장 중요한 부분은 membership function을 어떻게 정의할 것인가이다. <br>
최종적으로 QP solution을 구하기 전에, membership 생성을 필수적으로 해주어야 한다. <br>
실제로 이 방법론을 적용하기 위해서는 기본적인 모델을 적용하여 초기 신용점수 $s_k$를 구해야 한다. <br>
그 이후 신용점수를 간격 (0,1) 사이로 mapping 해줄 수 있는 membership function을 정의하고, 추가적으로 kernel function을 선택한 뒤 방법론을 적용하는 과정으로 이루어진다. <br>

논문에서 제시된 membership function은 다음과 같다. <br>

$$
\begin{aligned}
  Linear : m_k &= \frac{s_k-\min_{k} s_k} {\max_{k} s_k - \min_{k} s_k} \\
  Logistic : m_k &= \frac{1} {1+exp(-s_k)} \\
  probit : m_k &= \Phi^{-1}(\frac{s_k-\mu}{\sigma})
\end{aligned}
$$

(Bridge 함수도 있었는데 기호에 대한 설명이 없어서 이해하지 못했다.) <br>
신용점수 $s_k$를 (0,1)에 놓이도록 하는 다른 함수들을 사용해도 좋을 것이다.

<br>

여기서 아쉬운 점은 이 방법론에서 가장 중요한 부분인데도 설명이 다소 빈약하게 느껴졌다. <br>
kernel을 선택하는 데 어떤 이론적인 배경이 없는 것처럼 membership function도 마찬가지로 주관적이므로 다루기 어려웠으리라고 생각한다..!

<br>

> Empirical analysis 

new fuzzy SVM의 training step을 정리해보자면 다음과 같다.<br>

1. 기존의 다른 방법(로지스틱, standard SVM 등)을 적용하여 초기 신용점수를 구한다. <br>
2. 선택한 멤버십 함수로 + class의 $m_k$와 - class의 $1-m_k$를 구한다. <br>
3. 최종 classifier를 얻기 위해 QP solution을 구한다. (=new fuzzy SVM을 적용한다.)

논문에서는 SVM, fuzzy SVM, new fuzzy SVM을 kernel과 membership의 조합에 따라 모델 성능을 비교하고 있다. <br>
자세한 결과는 논문을 참고하길 바란다.

<br>

# Comment
> 의의

논문을 다 읽고 나서 수리통계학에서 나오는 UMVUE 개념이 생각이 났다. <br>
어떤 좋은 추정량이 있을 때 추가적인 성질을 고려해서 성능을 개선시킨다는 아이디어가 똑같이 적용되었다. <br>
모델도 마찬가지로 우수한 분류성능을 가지는 SVM을 가중오차합을 고려하는 방식으로 모델을 개선시켰다. <br>
역시 가중합, 가중평균 개념은 수학적으로 굉장히 중요한 개념이라는 생각이 든다.

고객의 피해를 최소한으로 하면서 기업의 이익을 높이기 위해 양쪽 클래스에 대한 가능성을 두어 리스크를 관리한다는 직관은 당연하면서도 굉장히 훌륭하다. <br>
2005년도에 나온 논문이니 지금은 더 많이 발전되어 있을 것이라 기대한다 !

<br>

> 한계점

방법론 자체는 데이터 활용도가 높다고도 볼 수 있지만 과적합에 대한 우려가 있을 수 있다. <br>
저자도 이를 고려한 것인지 다른 기본적인 모델을 이용한 초기 신용점수를 구하고, memvership을 할당한 뒤 fuzzy SVM을 적용시켜 모델을 개선하는 형태의 방법론을 제시했다. 
<br>
동일한 데이터에 대해 다른 모델을 써서 나온 결과를 동시에 이용하는 것은 과적합과 얼마나 연관이 있을까? <br>
(이 부분은 더 생각해봐야겠다.) <br>

무엇보다 가장 큰 문제점은 연산속도가 매우 느리다는 것이다. <br>
QP solution을 제대로 이해하고 있진 않지만 quadratic 문제라면 복잡하다는 생각이 드는데, 데이터를 2배로 늘려버리면 표본크기가 작더라도 오래 걸릴거라고 생각한다. <br>
논문 실험 파트에서도 데이터에 따라 편차는 있으나 분류 성능은 비슷해보였다. <br>
분류 정확도를 약간 높이기 위해 이렇게 시간과 비용이 많이 드는 이 모델을 실무에 잘 적용할 수 있을지는 의문이다.

<br>