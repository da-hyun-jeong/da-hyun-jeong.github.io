---
title: Chap6. Linear Model Selection and Regularization
excerpt: ISL 
toc: true
toc_sticky: true

author_profile: false

date: 2021-09-07 12:30:00 -0000
categories: 
  - ISL 
tags:
  - Machine Learning
---

<br>

이 장에서는 선형모형의 최소제곱법을 다른 적합법으로 대체해서 모델을 개선시키는 방법에 대해 알아본다. <br>
최소제곱법을 사용하지 않고 다른 방법을 사용하려는 이유는 무엇일까? <br>
이유는 다른 방법들을 사용했을 때 더 나은 예측 정확도와 모델 해석력을 제공할 수 있기 때문이다.

- 예측 정확도 : 반응변수와 설명변수들 사이의 상관관계가 거의 선형인 경우 최소제곱 추정치들은 편향이 적을 것이다. low-dimension 일 때는 낮은 분산을 가지는 경향이 있고, test set에 대해서도 좋은 성능을 낼 것이다. 그렇지만 n이 p보다 많이 크지 않으면 최소제곱법의 변동이 커지고 과적합 문제가 생긴다. high-dimension일 때는 더이상 unique 한 최소제곱추정량이 존재하지 않는다. 이 때는 계수들을 shrinkage 하는 방법을 이용한다.

- 모델 해석력 : 다중회귀모형에서 사용되는 변수들은 반응변수와 관련 없는 경우도 많다. 이러한 변수들을 제거하여 모델을 간소화시키기 위해 변수에 대응하는 계수 추정치를 0으로 만들어준다. 최소제곱법으로는 계수 추정치를 0으로 만들 수 없으므로 다른 방법을 이용해야 한다.

<br>

이 장에서 배우게 될 최소제곱법의 대안들은 다음과 같이 크게 세 가지가 있다.

- subset selection : $p$개의 설명변수 중에서 반응변수와 관련이 있다고 생각하는 부분집합을 찾아낸다. 이 부분집합만으로 최소제곱법을 적용하여 모델을 만든다.

- shrinkage : $p$개의 설명변수 모두를 포함하며 모델을 적합하긴 하지만 추정된 계수를 최소제곱 추정치와 비교해 0으로 수축시킨다. 이 방법은 분산을 줄여준다.

- Dimension Reduction : $p$개의 설명변수를 $M$차원 부분공간으로 투영하는 것이다. 이 공간은 $M$개 다른 선형결합 또는 투영을 계산함으로써 얻어진다. 이 $M$개의 설명변수로 선형회귀모형을 적합한다.

<br>
<br>

# 1. 부분집합 선택 (skip)

- best subset 선택

- forward selection

- backward elimination

- stepwise selection


<br>
<br>

# 2. Shrinkage 방법

계수 추정치들을 제한(constraints)하거나 정규화(regularizes)하여 $p$개의 설명변수를 모두 포함하는 모델을 적합할 수 있다. <br>
이 기법은 계수 추정치들을 0으로 수축(shrinkage)하는 것과 같다. <br>
계수 추정치들을 수축하는 것은 추정치들의 분산을 상당히 줄일 수 있는 것으로 밝혀져 있다. <br>
대표적인 방법론은 ridge와 lasso가 있다.

> Ridge

기존의 최소제곱법은 다음 식을 최소로 하는 $\beta_1, ..., \beta_p$를 구한다.

$$
RSS = \sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2
$$

Ridge는 $l_2$-norm penalty term이 들어간 다음의 식을 최소로 하는 $\hat{\beta}^R$를 구한다.

$$
\sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2 + \lambda\sum_{j=1}^p \beta_j^2
$$

여기서 $\lambda \geq 0$ 는 tunning parameter 이다. <br>
$\lambda$는 회귀계수들의 영향력을 조절하는 역할을 한다. <br>

<br>

최소제곱보다 ridge가 나은 이유는 bias-variance trade-off 때문이다. <br>
$\lambda$가 증가하면 ridge의 유연성이 감소하게 되어 분산은 감소하지만 편향은 증가한다. <br>
따라서 최소제곱 추정치의 분산이 크거나 $p > n$일 때 ridge를 사용하면 약간의 편향 증가로 분산을 크게 감소시킬 수 있다.

<br>

> Lasso

Ridge의 한 가지 단점은 모든 설명변수 $p$개를 포함하여 모델을 적합하므로 penalty term이 계수를 0에 가까이 수축시키지만 정확하게 0이 되지는 않는다. <br>
즉, 설명변수의 영향을 줄여주지만 아예 제외시키지는 않으므로 변수의 갯수가 많아질수록 모델을 해석하기 어려워진다. <br>
lasso는 이러한 단점을 보완한 방법으로 penalty term의 형태를 $l_1$-norm으로 변형시킨 추정량이다.

$$
\sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2 + \lambda\sum_{j=1}^p |\beta_j|
$$

ridge처럼 lasso는 계수 추정치들을 0으로 수축하고 $\lambda$가 커지면 정확히 0이 되도록 하므로 **변수선택**의 기능이 있다. <br>
따라서 **$sparse \; model$**을 제공하므로 ridge보다 모델을 해석하는 것이 훨씬 더 쉽다. <br>

<br>

> 최적화 관점

부분집합 선택, ridge, lasso를 최적화 형태로 나타내보면 밀접한 관련이 있다는 것을 알 수 있다.

$$
\begin{aligned}
\textrm{minimize}_{\beta} \; & \sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2 \; 
\textrm{subject to} \;  \sum_{j=1}^p I(\beta_j \neq 0) \leq s\\

\textrm{minimize}_{\beta} \; & \sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2 \; 
\textrm{subject to} \;  \sum_{j=1}^p |\beta_j| \leq s\\

\textrm{minimize}_{\beta} \; & \sum_{i=1}^n \left( y_i - \beta_0- \sum_{j=1}^p\beta_j x_{ij} \right) ^2 \; 
\textrm{subject to} \;  \sum_{j=1}^p \beta_j^2 \leq s\\  
\end{aligned}
$$

첫 번째 식은 최대 $s$개의 계수들이 0이 아니라는 조건 하에서 RSS를 가장 작게 하는 계수 추정치들의 집합을 찾는 것인데, 이는 부분집합 선택 방법과 동일한 문제이다. <br>
이 때 $p$가 크다면 이 식을 푸는 것은 고려해야 할 모형이 $\binom{p}{s}$개 이므로 계산상 실현 불가능하다. <br>
여기서 ridge와 lasso는 제약조건을 풀기 쉬운 형태로 대체한 것으로 생각할 수 있다.

<br>

> Lasso의 변수선택 성질

ridge와 달리 alsso가 정확하게 0이 되는 계수 추정치를 얻게 되는 이유는 무엇일까? <br>
$p$=2인 경우 최적화 솔루션을 구했을 때 그림을 그려보면 이해하기 쉽다.

<br>

<p align="center">
	<img src="https://user-images.githubusercontent.com/86343664/132493417-36a46936-4252-4852-807e-e4f92489906c.png">
</p>

<br>

점 $\hat{\beta}$은 최소제곱 추정치이며 이 점를 중심으로 한 타원들은 일정한 RSS의 영역을 나타낸다. <br>
주어진 타원 위의 모든 점들은 동일한 RSS 값을 갖는다. <br>
타원이 최소제곱추정치로부터 멀리 퍼지면 RSS가 증가한다. <br>

ridge의 제약조건은 $|\beta_1|+|\beta_2| \leq s$ 인 마름모가 되고, lasso의 제약조건은 $\beta_1^2+\beta_2^2 \leq s$ 인 원이 된다. <br>
ridge와 lasso의 추정치는 어떤 타원이 제한영역 처음으로 만나는 점에 의해 주어진다. <br>
ridge는 원형 형태이므로 교점이 $\beta$축에 없으므로 계수 추정치는 0이 될 수 없다. <br>
하지만 lasso의 제한영역은 각 $\beta$축에 모서리가 있어 타원과 교점이 생긴다. <br>
따라서 $\beta_1$, $\beta_2$ 중 하나는 0이 된다. <br>
즉, 그림에서는 교점이 $\beta_1$=0에서 발생하므로 모델은 $\beta_2$만을 포함하게 된다. <br>

$\lambda$=0이 되면 제한영역이 $\hat{\beta}$를 포함하게 되고, 최소제곱 추정치와 같아지게 된다. <br>
하지만 그림에서 보는 것처럼 최소제곱추정치는 마름모와 원의 바깥에 있으므로 ridge와 lasso의 추정치와 다르다. 

<br>
<br>

# 3. 차원축소 방법

지금까지 살펴본 방법들은 원래 변수들의 부분집합을 사용하거나 계수들을 0으로 수축하는 두 가지 방식으로 분산을 제어했다. <br>
이제 설명변수들을 **변환**한 변수들을 사용해 최소제곱법을 적합하는 방법들을 알아본다. <br>
이러한 방법들을 차원축소 방법이라 한다.

<br>

원래의 $p$개 설명변수들의 $M < p$개 선형결합을 $Z_1, ...,Z_M$이라 하자. <br>
즉, 어떤 상수들 $\phi_{1m},...,\phi_{pm}$에 대해 다음과 같이 표현된다.

$$
Z_m = \sum_{j=1}^p \phi_{jm}X_j \quad \text{for} \; m=1,...,M
$$

그 다음 이 변수들을 이용하여 최소제곱법을 적합한 후 선형회귀모형을 얻을 수 있다.

$$
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \quad i=1,...,n
$$

위 식에서 회귀계수들은 $\theta_0, \theta_1, ..., \theta_M$으로 주어진다. <br>
상수들 ** $\phi_{1m}, ..., \phi_{pm}$ **가 적절하게 선택되면 기존의 $\beta$를 사용하여 최소제곱법을 적합한 것보다 더 좋은 성능을 낼 것이다.

<br>

위의 두 식을 이용해 기존의 선형회귀모형과의 관계를 알 수 있다.

$$
\sum_{m=1}^M \theta_m z_{im} = \sum_{m=1}^M \theta_m \sum_{j=1}^p \phi_{jm}x_{ij} = \sum_{j=1}^p \sum_{m=1}^M \theta_m\phi_{jm}x_{ij} = \sum_{j=1}^p \beta_j x_{ij}
$$

여기서 $\beta_j = \sum_{m=1}^M \theta_m\phi_{jm}$이다. <br>
따라서 이 모형은 원래의 선형모형의 특수한 경우로 생각할 수 있다.

<br>

차원축소는 추정된 $\beta_j$들이 위와 같은 형태를 가져야 하므로 계수들의 형태를 제한하는 역할을 한다. <br>
이로 인해 추정치가 편향될 수 있지만, $M \ll p$인 경우 계수들의 분산을 현저하게 줄일 수 있다.

<br>

모든 차원축소 방법은 2단계로 진행된다. <br>
- 변환된 설명변수 $Z_1, .., Z_M$ 구하기
- $M$개의 설명변수를 사용해 모델 적합하기

여기서 $Z_1, .., Z_M$의 선택, $\phi_{jm}$의 선택에 따라 방법이 다양해지는 것이다. <br>
이 장에서는 **주성분(principal components)**과 **부분최소제곱(partial least squares)** 두 가지를 고려한다.

<br>

> 주성분회귀(principal componenets regression)

주성분분석($Principal \; Components \; Analysis$)는 비지도학습 단원에서 자세하게 다루기로 하고, 여기서는 회귀를 위한 차원축소 기법으로 사용하는 것에 대해서만 설명한다. 

<br>

주성분회귀는 $X_1,...,X_p$의 변동이 큰 방향들이 $Y$와 관련이 있는 방향이라고 가정한다. <br>
분산은 데이터의 정보를 담고 있고, 이 가정이 항상 옳지는 않겠지만 충분히 합리적이라는 것은 판명되었다고 한다. <br>
처음 $M$개의 주성분 $Z_1,...,Z_M$을 설명변수로 해서 최소제곱법을 적합시킨다. <br>
PCR의 가정이 성립한다면 반응변수와 관련된 데이터의 정보가 주성분에 모두 들어있으므로 과적합을 줄일 수 있고, 더 나은 결과를 얻을 수 있다.

 <br>

> 부분최소제곱(Partial Least Squares)

PCR 기법은 설명변수 $X_1, ...,X_p$를 가장 잘 나타내는 선형결합 또는 방향을 찾아내는 것이다. <br>
PCR은 주성분 방향을 찾을 때 반응변수 $Y$를 사용하지 않으므로 비지도학습이다. <br>
그 결과, PCR은 설명변수들을 가장 잘 설명하는 방향이 반응변수를 예측하는데 사용하기에도 가장 좋은 방향이 된다는 보장이 없다. 

<br>

이에 대한 대안으로 부분최소제곱 PLS에 대해 살펴보자. <br>
PCR처럼 원래 변수들의 선형결합인 새로운 변수들 $Z_1,...,Z_M$의 집합을 찾고 이 $M$개 새로운 변수들을 이용한 최소제곱을 통해 선형모델을 적합한다. <br>
다른 점은 반응변수 $Y$도 이용하여 supervised way로 찾는다. <br>

예를 들어 첫 번째 PLS 방향이 어떻게 계산되는지 설명해보면, <br>
$p$개의 설명변수들을 표준화한 후, $\phi_{j1}$ 각각을 $Y$와 $X_j$에 대한 단순회귀 계수와 동일하게 두어 $Z_1$을 계산한다. <br>
따라서 $Z_1=\sum_{j=1}^p \phi_{ji}X_j$ 계산에서 반응변수와 가장 강하게 관련되어 있는 변수들에 가장 높은 가중치를 부여한다. <br>
두 번째 PLS 방향은 $Z_1$에 대해 각 변수의 회귀를 적합하고 얻어진 잔차로 $Y$를 조정하여 $Z_1$과 '직교'인 데이터를 만든다. <br>
다시 같은 방식으로 $Z_2$를 계산한다. <br>
이렇게 $M$번 반복해서 얻은 PLS 성분 $Z_1,...,Z_M$을 구해서 최소제곱법으로 선형모형을 적합한다. <br>
PLS는 편향을 줄일 수 있지만 분산을 증가시킬 수 있으므로 PCR과 비교해서 특별한 장점은 없다.

<br>
<br>

# 4. 고차원인 경우

고차원 데이터는 관측치보다 더 많은 변수가 있는 경우를 말한다. <br>
$n > p$일 때 발생하는 bias-variance trade-off, 과적합 문제는 고차원에서 더욱 중요해진다. <br>

- $n < p$이면 모델이 너무 유연해지므로 과적합이 발생하여 test set에 대한 성능이 매우 떨어진다.
- 모델의 적합도를 측정하는 $C_P$, AIC, BIC 등은 $\hat{\sigma}^2$=0으로 추정하게 된다 . 이 떄 MSE=0 이거나 $R^2$=1가 되는 경우가 발생하므로 고차원 설정에 더 잘 맞는 척도가 필요하다.